{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Machine-Learning TD\n",
                "By Corentin Meyer, PhD Student @ CSTB - iCube, 04/10 ESBS\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# PART 1: Exploration, formating and data split\n",
                "\n",
                "## The Data that we will use\n",
                "## [Stroke Prediction Dataset](https://www.kaggle.com/fedesoriano/stroke-prediction-dataset)\n",
                "### **Context**\n",
                "According to the World Health Organization (WHO) stroke is the 2nd leading cause of death globally, responsible for approximately 11% of total deaths.\n",
                "This dataset is used to predict whether a patient is likely to get stroke based on the input parameters like gender, age, various diseases, and smoking status. Each row in the data provides relavant information about the patient.\n",
                "Attribute Information\n",
                "### **11 clinical features for predicting stroke events**\n",
                "1. **id:** unique identifier\n",
                "2. **gender:** \"Male\", \"Female\" or \"Other\"\n",
                "3. **age**: age of the patient\n",
                "4. **hypertension**: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension\n",
                "5. **heart_disease**: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease\n",
                "6. **ever_married**: \"No\" or \"Yes\"\n",
                "7. **work_type**: \"children\", \"Govt_jov\", \"Never_worked\", \"Private\" or \"Self-employed\"\n",
                "8. **Residence_type**: \"Rural\" or \"Urban\"\n",
                "9. **avg_glucose_level**: average glucose level in blood\n",
                "10. **bmi**: body mass index\n",
                "11. **smoking_status**: \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"*\n",
                "12. **stroke**: 1 if the patient had a stroke or 0 if not  \n",
                "* Note: \"Unknown\" in smoking_status means that the information is unavailable for this patient"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## **Tasks: Data Exploration**\n",
                "1. Import the data (and set the dataframe index to the ID column)\n",
                "2. Print the shape of the dataset and the first 5 lines\n",
                "3. Calculate the ratio stroke/non-stroke  \n",
                "4. Plot histogram of the \"age\" column separate by stroke vs non-stroke status.\n",
                "5. Print the type of each columns (number, object...)"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## **Questions:**\n",
                "1. How many entries (patients) are in the dataset ?\n",
                "2. How many columns (features) ?\n",
                "3. Plot the histogram of the age feature. Do separate histogram for stroke vs non-stroke patients\n",
                "4. What is the percentage of the patients that had a stroke ?\n",
                "5. Show the type of data in each columns. What type of processing will we have to do for each type ?"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "import warnings\n",
                "warnings.filterwarnings('ignore')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "import pandas as pd\n",
                "\n",
                "# Import the data using read_csv() and set the index with set_index()\n",
                "df = pd.read_csv(\"healthcare-dataset-stroke-data.csv\")\n",
                "df.set_index(\"id\", inplace=True)\n",
                "\n",
                "# Print the shape of the dataframe and the head using df.shape and df.head()\n",
                "print(df.shape)\n",
                "df.head()"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "(5110, 11)\n"
                    ]
                },
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>gender</th>\n",
                            "      <th>age</th>\n",
                            "      <th>hypertension</th>\n",
                            "      <th>heart_disease</th>\n",
                            "      <th>ever_married</th>\n",
                            "      <th>work_type</th>\n",
                            "      <th>Residence_type</th>\n",
                            "      <th>avg_glucose_level</th>\n",
                            "      <th>bmi</th>\n",
                            "      <th>smoking_status</th>\n",
                            "      <th>stroke</th>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>id</th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>9046</th>\n",
                            "      <td>Male</td>\n",
                            "      <td>67.0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>Yes</td>\n",
                            "      <td>Private</td>\n",
                            "      <td>Urban</td>\n",
                            "      <td>228.69</td>\n",
                            "      <td>36.6</td>\n",
                            "      <td>formerly smoked</td>\n",
                            "      <td>1</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>51676</th>\n",
                            "      <td>Female</td>\n",
                            "      <td>61.0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>Yes</td>\n",
                            "      <td>Self-employed</td>\n",
                            "      <td>Rural</td>\n",
                            "      <td>202.21</td>\n",
                            "      <td>NaN</td>\n",
                            "      <td>never smoked</td>\n",
                            "      <td>1</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>31112</th>\n",
                            "      <td>Male</td>\n",
                            "      <td>80.0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>Yes</td>\n",
                            "      <td>Private</td>\n",
                            "      <td>Rural</td>\n",
                            "      <td>105.92</td>\n",
                            "      <td>32.5</td>\n",
                            "      <td>never smoked</td>\n",
                            "      <td>1</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>60182</th>\n",
                            "      <td>Female</td>\n",
                            "      <td>49.0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>Yes</td>\n",
                            "      <td>Private</td>\n",
                            "      <td>Urban</td>\n",
                            "      <td>171.23</td>\n",
                            "      <td>34.4</td>\n",
                            "      <td>smokes</td>\n",
                            "      <td>1</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1665</th>\n",
                            "      <td>Female</td>\n",
                            "      <td>79.0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>Yes</td>\n",
                            "      <td>Self-employed</td>\n",
                            "      <td>Rural</td>\n",
                            "      <td>174.12</td>\n",
                            "      <td>24.0</td>\n",
                            "      <td>never smoked</td>\n",
                            "      <td>1</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "       gender   age  hypertension  heart_disease ever_married      work_type  \\\n",
                            "id                                                                             \n",
                            "9046     Male  67.0             0              1          Yes        Private   \n",
                            "51676  Female  61.0             0              0          Yes  Self-employed   \n",
                            "31112    Male  80.0             0              1          Yes        Private   \n",
                            "60182  Female  49.0             0              0          Yes        Private   \n",
                            "1665   Female  79.0             1              0          Yes  Self-employed   \n",
                            "\n",
                            "      Residence_type  avg_glucose_level   bmi   smoking_status  stroke  \n",
                            "id                                                                      \n",
                            "9046           Urban             228.69  36.6  formerly smoked       1  \n",
                            "51676          Rural             202.21   NaN     never smoked       1  \n",
                            "31112          Rural             105.92  32.5     never smoked       1  \n",
                            "60182          Urban             171.23  34.4           smokes       1  \n",
                            "1665           Rural             174.12  24.0     never smoked       1  "
                        ]
                    },
                    "metadata": {},
                    "execution_count": 2
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "# Use value_counts() on the \"stroke\" column.\n",
                "print(df[\"stroke\"].value_counts(normalize=True))\n",
                "\n",
                "# Use pandas built-in dataframe .hist() methods to plot the histo of the \"age\" column\n",
                "df.hist(\"age\", by=\"stroke\")"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "0    0.951272\n",
                        "1    0.048728\n",
                        "Name: stroke, dtype: float64\n"
                    ]
                },
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "array([<AxesSubplot:title={'center':'0'}>,\n",
                            "       <AxesSubplot:title={'center':'1'}>], dtype=object)"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 3
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "image/png": "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",
                        "text/plain": [
                            "<Figure size 432x288 with 2 Axes>"
                        ]
                    },
                    "metadata": {
                        "needs_background": "light"
                    }
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "# Print the \"dtypes\" attribute of dataframe\n",
                "df.dtypes"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "gender                object\n",
                            "age                  float64\n",
                            "hypertension           int64\n",
                            "heart_disease          int64\n",
                            "ever_married          object\n",
                            "work_type             object\n",
                            "Residence_type        object\n",
                            "avg_glucose_level    float64\n",
                            "bmi                  float64\n",
                            "smoking_status        object\n",
                            "stroke                 int64\n",
                            "dtype: object"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 4
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## **Tasks: Data Formating**\n",
                "You will now do the pre-processing of data.\n",
                "For each type of object you will have to process them in a usable format for ML algorithm.\n",
                "\n",
                "## **Questions**\n",
                "1. What columns are categorical data, what columns are numeric.\n",
                "2. What columns are already ready to be used and needs no change.\n",
                "3. What type of processing do you need to do on categorical data and why\n",
                "4. What type of processing do you need to do on numeric data and why\n",
                "5. What columns contains missing data ? What type of processing do you need to do in this case.\n",
                "\n",
                "## **Ressources**:\n",
                "[Google's Class: Transforming Numeric Data](https://developers.google.com/machine-learning/data-prep/transform/transform-numeric)  \n",
                "[Google's Class: Transforming Categorical Data](https://developers.google.com/machine-learning/data-prep/transform/transform-categorical)"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "![Categorial Feature](https://i.imgur.com/mRFk0Sh.png)"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "source": [
                "import numpy as np\n",
                "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
                "from sklearn.experimental import enable_iterative_imputer\n",
                "from sklearn.impute import IterativeImputer\n",
                "\n",
                "# Create lists with columns names for each processing categories\n",
                "columns_nothing = [\"hypertension\", \"heart_disease\", \"stroke\"]\n",
                "columns_categorical = [\"gender\",\"ever_married\", \"work_type\", \"Residence_type\", \"smoking_status\"]\n",
                "columns_numeric = [\"age\", \"avg_glucose_level\",\"bmi\"]\n",
                "\n",
                "# Nothing-To-Do Cols: just select them and convert the pandas dataframe to numpy array ( to_numpy() )\n",
                "X_nothing_to_do = df[columns_nothing]\n",
                "X_nothing_to_do = X_nothing_to_do.to_numpy()\n",
                "\n",
                "# Categorial data to numeric: do one hot encoding ( OneHotEncoder(), .fit_transform and .toarray() )\n",
                "enc = OneHotEncoder()\n",
                "X_cat = df[columns_categorical]\n",
                "X_cat_onehot = enc.fit_transform(X_cat).toarray()\n",
                "X_cat_columns_onehot = enc.get_feature_names()\n",
                "\n",
                "# Numeric data: do scaling (0-1) (StandardScaler() and .fit_transform())\n",
                "X_num = df[columns_numeric]\n",
                "X_num_scaled = StandardScaler().fit_transform(X_num)\n",
                "X_num_columns_scaled = X_num.columns\n",
                "\n",
                "# Combine the nothing_to_do_columns, one-hot cols, and numeric data into a numpy array\n",
                "# using np.concatenate\n",
                "array_data = np.concatenate((X_cat_onehot, X_num_scaled, X_nothing_to_do), axis=1)\n",
                "\n",
                "# Missing Data: Use imputer to predict them ( IterativeImputer() and .fit_transform() )\n",
                "imp_mean = IterativeImputer(random_state=777)\n",
                "imp_mean.fit(array_data)\n",
                "array_data = imp_mean.transform(array_data)\n",
                "\n",
                "# You can recreate a DataFrame for pretty printing, but this is optional.\n",
                "df_data  = pd.DataFrame(data=array_data, columns=list(X_cat_columns_onehot) + list(X_num_columns_scaled) + columns_nothing)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## **Tasks: Data Train/Test Spliting**\n",
                "Now you can split the data between training and testing data.\n",
                "\n",
                "## **Questions**\n",
                "1. What train/test ratio should you use.\n",
                "2. How many entries are in your train dataset and in your test dataset.\n",
                "3. Verify that you have the same stroke / no-stroke ratio between train and test dataset."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "from sklearn.model_selection import train_test_split\n",
                "# From your numpy array that you created, separate the X columns (features) and the Y column (label) in two variables\n",
                "X = array_data[:,:-1]\n",
                "Y = array_data[:,-1]\n",
                "\n",
                "# Use train_test_split() using X and Y with the ratio you selected.\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=777)\n",
                "\n",
                "# Print X_train and X_test shape, use np.unique on y_train, x_test to get the 0/1 ratio\n",
                "print(X_train.shape)\n",
                "print(X_test.shape)\n",
                "print(np.unique(y_train, return_counts=True)[1])\n",
                "print(np.unique(y_test, return_counts=True)[1])"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "(3066, 21)\n",
                        "(2044, 21)\n",
                        "[2925  141]\n",
                        "[1936  108]\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# PART 2: Create your machine-learning model"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## **Tasks: Choose a model and do basic evaluation**\n",
                "Select from scikit-learn a model and train it (fit) with the train data. Then calculate the accuracy of the model on the test data. Plot the confusion matrix of the test data classification.\n",
                "\n",
                "## **Questions:**\n",
                "1. Which model did you choose and why ? Have you set any particular (hyper)parameters ?\n",
                "2. What accuracy-score do you get and what conclusion can you take ?\n",
                "3. What do you observe on the confusion matrix and what conclusion can you take ?"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "![Accuracy and Confusion Matrix](https://i.imgur.com/EAf5SNh.png)"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import plot_confusion_matrix\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Select a model from: https://scikit-learn.org/stable/supervised_learning.html#supervised-learning\n",
                "\n",
                "# For example: create a clf variable containing a RandomForestClassifier(). \n",
                "# Don't hesitate to tweak its parameters as you like ! You can experiment.\n",
                "# Use fit() on clf variable with your x_train and y_train to train the model.\n",
                "clf = \n",
                "\n",
                "# Using the .score() method of clf, print its accuracy on x_test, y_test\n",
                "print()\n",
                "# Plot the confusion matrix using plot_confusion_matrix with clf, x_test, y_test)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## **Tasks: Correct the previous issue**\n",
                "You will now try to downsample your majority class to the level of minority class to have a 50/50 ratio and re-do a train/test split.  \n",
                "Then you will re-train a new model with the new ratio-corrected data and get accuracy+confusion matrix plot.\n",
                "Don't only predict the class, but also show the prediction probability for all data in the test set !\n",
                "## **Questions**\n",
                "1. What accuracy-score do you get with the new model and what conclusion can you take.\n",
                "2. What do you observe on the confusion matrix and what conclusion can you take.\n",
                "3. Did you managed to print the probability of each prediction ? What's the shape of the prediction probability output ? Is there a high variance between the different test entries in probability ?"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "![Downsampling](https://i.imgur.com/QPOuSKk.png)"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Using this tutorial: https://chrisalbon.com/code/machine_learning/preprocessing_structured_data/handling_imbalanced_classes_with_downsampling/\n",
                "# We will downsample our data by using the code in the \"Downsample Majority Class To Match Minority Class\" section.\n",
                "# Remember to switch 0 and 1 from the tutorial as here our dominant class is 0 and not 1.\n",
                "np.random.seed(777)\n",
                "i_class0 = \n",
                "i_class1 = \n",
                "# Number of observations in each class\n",
                "n_class0 = \n",
                "n_class1 = \n",
                "# For every observation of class 1, randomly sample from class 0 without replacement\n",
                "i_class0_downsampled = \n",
                "# Join together class 1's target vector with the downsampled class 0's target vector\n",
                "Y_down = \n",
                "X_down = \n",
                "\n",
                "# Re-do a train/test split but this time on the new X_down and Y_down data (downsampled).\n",
                "# Call the new variables X_train_down, X_test_down, y_train_down, y_test_down for example.\n",
                "X_train_down, X_test_down, y_train_down, y_test_down = \n",
                "\n",
                "# Recreate a model called clf_down with RandomForestClassifier()\n",
                "# And fit() it this time on X_train_down, y_train_down\n",
                "clf_down = \n",
                "\n",
                "# Calculate its accuracy using the .score() methods on X_test_down, y_test_down\n",
                "print()\n",
                "# Plot the confusion matrix as before, but with clf_down, X_test_down, y_test_down\n",
                "\n",
                "# Print the prediction probablity of the first 10 test data points using .predict_proba of the model.\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# PART 3: Evaluate your model"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## **Tasks: See all common metrics and evalute both models !**\n",
                "In this last part you will have to calculate all relevant metrics for a binary classification to compare your two models.  \n",
                "Make a table containing the results for both models in terms of: accuracy, balance accuracy, F1 Score, sensitivity (recall), specificity, Precision and confusion matrix data (True Pos., True Neg., False Pos., False Neg.)\n",
                "\n",
                "## **Questions:**\n",
                "1. Which model have the accuracy ?\n",
                "2. Which model have the best area under the curve (AUC) for the ROC-curve ?\n",
                "3. Which model have the best F1-Score and sensitivity ?\n",
                "4. Eventually, which model is better according to you ?"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "![Roc Curve](https://i.imgur.com/DFw604d.png)\n",
                "![F1-Score](https://i.imgur.com/8b5AkS3.png)"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Calculate all relevant metrics for a binary classification\n",
                "# Using Scikit score function: https://scikit-learn.org/stable/modules/model_evaluation.html\n",
                "# Calculate: Accuracy, Balanced Accuracy, F1 Score, Sensitivity (Recall), Specificity, Precision, TP TN FP FN\n",
                "# Also plot the ROC Curve and the Precision-Recall Curve using scikit built-in function\n",
                "\n",
                "from sklearn.metrics import accuracy_score\n",
                "from sklearn.metrics import balanced_accuracy_score\n",
                "from sklearn.metrics import recall_score\n",
                "from sklearn.metrics import confusion_matrix\n",
                "from sklearn.metrics import precision_score\n",
                "from sklearn.metrics import f1_score\n",
                "from sklearn.metrics import plot_precision_recall_curve, plot_roc_curve\n",
                "\n",
                "def get_all_metrics(clf, X_test, y_test):\n",
                "\t\"\"\"\"Function that returns all the metrics for a given classifier and test data\"\"\"\n",
                "\t# Use classifier to predict the Y label of X_test data with clf.predict()\n",
                "\ty_pred = \n",
                "\n",
                "\t# Confusion Matrix results\n",
                "\ttn, fp, fn, tp = \n",
                "\n",
                "\t# Calculate the metrics: accuracy, balanced accuracy, recall, precision, F1-score, specificity\n",
                "\tac = \n",
                "\tbac = \n",
                "\tre = \n",
                "\tpr = \n",
                "\tf1 = \n",
                "\tsp = \n",
                "\n",
                "\treturn [bac, ac, f1, re, sp, pr, tp, tn, fp, fn]\n",
                "\n",
                "def make_table_clf(clf, clf_down, X_test, y_test, X_test_down, y_test_down):\n",
                "\t\"\"\"\"Use the get_all_metrics function to make a table to compare two models\"\"\"\n",
                "\tresults = get_all_metrics(\"...\") # Fill\n",
                "\tresults_down = get_all_metrics(\"...\") # Fill\n",
                "\tdf = pd.DataFrame([[i for i in results], [j for j in results_down]], columns=[\"Balanced-Accuracy\", \"Accuracy\", \"F1-Score\", \"Sensitivity (Recall)\", \"Specificity\", \"Precision\", \"TP\",\" TN\", \"FP\", \"FN\"], index=[\"CLF\",\"CLF DownSampled\"])\n",
                "\treturn df"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "make_table_clf(\"...\") # Fill with clf, clf down, and its respective X and Y data"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "print(\"ROC Curve and Prec/Recall Cruve Results For Initial Classifier: \")\n",
                "plot_precision_recall_curve(\"...\") # Fill with clf, X and Y data\n",
                "plot_roc_curve(\"...\") # Fill with clf, X and Y data"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "print(\"ROC Curve and Prec/Recall Cruve Results For Downsampled Classifier: \")\n",
                "plot_precision_recall_curve(\"...\") # Fill with clf_down, X_down and Y_down data\n",
                "plot_roc_curve(\"...\") # Fill with clf_down, X_down and Y_down data"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# BONUS 1: Do Cross-validation instead of simple test/train split."
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## **Tasks:**  \n",
                "Instead of a simple Test/Train split, we will do cross-validation. This means that we will train multiple models with different splits, so that all data have been used for training and all for testing. Then we will average the results of all models.  \n",
                "For example, if we do a 80/20% train/test split, then we would need to do a 5 fold cross-validation so that each data has been in the test-set at least once.  \n",
                "Find a way to do \"stratified k fold\", calculate \"cross val score\" with scikit and print the mean and standard deviation of the score.\n",
                "## **Questions:**  \n",
                "1. What is the point of cross-validation ? Did it increase performance ? If not, what is it useful for ?"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "![Cross-validation](https://fr.mathworks.com/discovery/cross-validation/_jcr_content/mainParsys/image.adapt.full.medium.jpg/1623131646985.jpg)"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "from sklearn.model_selection import StratifiedKFold\n",
                "from sklearn.model_selection import cross_val_score\n",
                "\n",
                "# Create a cv variable using the StratifiedKFold with 5 folds.\n",
                "cv = \n",
                "# Use the cross_val_score with your clf_down model, your train data, cv varaible and select f1 as scoring method.\n",
                "# Store the results in a cross_scores variables\n",
                "cross_scores = cross_val_score(\"....\")\n",
                "# Print the full cross validation scores, calculate the mean and the standard deviation of these 5 folds.\n",
                "print()\n",
                "print()\n",
                "print()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# BONUS 2:  Automatic hyper-parameters tuning"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## **Tasks:**\n",
                "We will use optuna to automatically find the best parameters for the chosen ML Algorithm (Random Forest here). We define a number of trials (50), a scoring metrics to maximize (F1 Score) and a parameters space to explore (params_grid).  \n",
                "In the end we will have a model with the best parameters that have been found and we will compared its metrics to our previous classifier (clf_down) without optimisation.  \n",
                "Try to tweak the hyper parameters spaces to shrink or expand it. Try to optimize another ML Algorithm.  \n",
                "## **Questions:**  \n",
                "1. What are the best parameters detected ? Are your best parameters different from the one of other students ? Why ?\n",
                "2. Did the metrics improved ? Was the optimisation useful ?\n",
                "\n",
                "## **Warnings**\n",
                "This is a pretty long task depending of your computed because the program will train and test 50+ models with differents parameters. Your computer will heat up a bit for a few minutes, so don't run it if you don't want it to happens !"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "![Hyerparameters Tuning](https://i.imgur.com/3plYcqn.png)"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# If you are using Google Colab, uncomment the next line starting with ! and run the cell to install optuna\n",
                "# !pip install optuna"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "import optuna\n",
                "from sklearn.base import clone\n",
                "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
                "\n",
                "def make_table_clf_opti(clf, clf_down, X_test, y_test, X_test_down, y_test_down):\n",
                "\tresults = get_all_metrics(clf, X_test, y_test)\n",
                "\tresults_down = get_all_metrics(clf_down, X_test_down, y_test_down)\n",
                "\tdf = pd.DataFrame([[i for i in results], [j for j in results_down]], columns=[\"Balanced-Accuracy\", \"Accuracy\", \"F1-Score\", \"Sensitivity (Recall)\", \"Specificity\", \"Precision\", \"TP\",\" TN\", \"FP\", \"FN\"], index=[\"CLF Down\",\"CLF Down Optimized\"])\n",
                "\treturn df\n",
                "\n",
                "def objective_RF(\n",
                "    trial, est, x_train, y_train, param_grid, scoring_metric):\n",
                "    \"\"\"\n",
                "    A function that is used by Optuna to get the scoring of a model given the parameters of a specific trial.\n",
                "    \"\"\"\n",
                "    params = {\n",
                "        \"n_estimators\": trial.suggest_int(\n",
                "            \"n_estimators\", param_grid[\"n_estimators\"][0], param_grid[\"n_estimators\"][1]\n",
                "        ),\n",
                "        \"criterion\": trial.suggest_categorical(\"criterion\", param_grid[\"criterion\"]),\n",
                "        \"max_depth\": trial.suggest_int(\n",
                "            \"max_depth\", param_grid[\"max_depth\"][0], param_grid[\"max_depth\"][1]\n",
                "        ),\n",
                "        \"min_samples_split\": trial.suggest_int(\n",
                "            \"min_samples_split\",\n",
                "            param_grid[\"min_samples_split\"][0],\n",
                "            param_grid[\"min_samples_split\"][1],\n",
                "        ),\n",
                "        \"min_samples_leaf\": trial.suggest_int(\n",
                "            \"min_samples_leaf\",\n",
                "            param_grid[\"min_samples_leaf\"][0],\n",
                "            param_grid[\"min_samples_leaf\"][1],\n",
                "        ),\n",
                "        \"max_features\": trial.suggest_categorical(\n",
                "            \"max_features\", param_grid[\"max_features\"]\n",
                "        ),\n",
                "        \"bootstrap\": trial.suggest_categorical(\"bootstrap\", param_grid[\"bootstrap\"]),\n",
                "        \"oob_score\": trial.suggest_categorical(\"oob_score\", param_grid[\"oob_score\"]),\n",
                "        \"n_jobs\": trial.suggest_categorical(\"n_jobs\", param_grid[\"n_jobs\"]),\n",
                "        \"class_weight\": trial.suggest_categorical(\n",
                "            \"class_weight\", param_grid[\"class_weight\"]\n",
                "        ),\n",
                "    }\n",
                "    cv = StratifiedKFold(n_splits=5, shuffle=True)\n",
                "    model = clone(est).set_params(**params)\n",
                "    performance = np.mean(\n",
                "        cross_val_score(\n",
                "            model,\n",
                "            x_train,\n",
                "            y_train,\n",
                "            cv=cv,\n",
                "            scoring=scoring_metric,\n",
                "            error_score=\"raise\",\n",
                "        )\n",
                "    )\n",
                "    return performance\n",
                "\n",
                "# Random Forest parameters space\n",
                "param_grid = {'n_estimators': [10,400],\n",
                "                'criterion' : ['gini', 'entropy'],\n",
                "                'max_depth' : [1, 15],\n",
                "                'min_samples_split' : [2, 50], \n",
                "                'min_samples_leaf' : [1, 50],\n",
                "                'max_features' : [None, 'auto','log2'],\n",
                "                'bootstrap' : [True],\n",
                "                'oob_score' : [False, True],\n",
                "                'n_jobs' : [-1],\n",
                "                'class_weight' : [None, 'balanced']}\n",
                "\n",
                "# Run Hyperparameter Sweep: first we set up some settings.\n",
                "n_trials = 50\n",
                "scoring_metric = \"f1\"\n",
                "timeout = 300 \n",
                "est = RandomForestClassifier()\n",
                "sampler = optuna.samplers.TPESampler()\n",
                "study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
                "optuna.logging.set_verbosity(optuna.logging.CRITICAL)\n",
                "\n",
                "# We launch the optimization using the previous function as score-returning.\n",
                "study.optimize(\n",
                "    lambda trial: objective_RF(\n",
                "        trial, est, X_train_down, y_train_down, param_grid, scoring_metric),\n",
                "    n_trials=n_trials,\n",
                "    timeout=timeout,\n",
                "    catch=(ValueError,),\n",
                ")\n",
                "# We retrieve the best results now that the optimization is finished.\n",
                "print(\"Best trial:\")\n",
                "best_trial = study.best_trial\n",
                "print(\"  Score: \", best_trial.value)\n",
                "print(\"  Params: \")\n",
                "for key, value in best_trial.params.items():\n",
                "    print(\"    {}: {}\".format(key, value))\n",
                "\n",
                "# Train model using 'best' hyperparameters\n",
                "est = RandomForestClassifier()\n",
                "clf = clone(est).set_params(**best_trial.params)\n",
                "\n",
                "model = clf.fit(X_train_down, y_train_down)\n",
                "y_pred = clf.predict(X_test_down)\n",
                "\n",
                "plot_precision_recall_curve(model, X_test_down, y_test_down)\n",
                "plot_roc_curve(model, X_test_down, y_test_down)\n",
                "make_table_clf_opti(clf_down, model, X_test_down, y_test_down, X_test_down, y_test_down)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Additional Ressources\n",
                "\n",
                "Google's Class lists and ressources on ML [https://developers.google.com/machine-learning/crash-course](https://developers.google.com/machine-learning/crash-course)  \n",
                "If you want to become an AI Expert: AI Rodmap [https://i.am.ai/roadmap](https://i.am.ai/roadmap)  \n",
                "Visualisation in Python [https://www.python-graph-gallery.com/](https://www.python-graph-gallery.com/)  \n",
                "Public Dataset to create projects [https://github.com/awesomedata/awesome-public-datasets](https://github.com/awesomedata/awesome-public-datasets)  \n",
                "Machine Learning Competitions with Prize [https://www.kaggle.com/](https://www.kaggle.com/)"
            ],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.9.5",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.9.5 64-bit ('TD_ML': conda)"
        },
        "interpreter": {
            "hash": "b1f4f77d01ceeeb3f1b50751faca703c6ff6e53a2ece845b812c7f39fafa84f3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}