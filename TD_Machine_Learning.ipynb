{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Machine-Learning TD\n",
                "By Corentin Meyer, PhD Student @ CSTB - iCube, 04/10 ESBS\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# PART 1: Import, format and split the data\n",
                "\n",
                "## The Data that we will use\n",
                "## [Stroke Prediction Dataset](https://www.kaggle.com/fedesoriano/stroke-prediction-dataset)\n",
                "### **Context**\n",
                "According to the World Health Organization (WHO) stroke is the 2nd leading cause of death globally, responsible for approximately 11% of total deaths.\n",
                "This dataset is used to predict whether a patient is likely to get stroke based on the input parameters like gender, age, various diseases, and smoking status. Each row in the data provides relavant information about the patient.\n",
                "Attribute Information\n",
                "### **11 clinical features for predicting stroke events**\n",
                "1. **id:** unique identifier\n",
                "2. **gender:** \"Male\", \"Female\" or \"Other\"\n",
                "3. **age**: age of the patient\n",
                "4. **hypertension**: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension\n",
                "5. **heart_disease**: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease\n",
                "6. **ever_married**: \"No\" or \"Yes\"\n",
                "7. **work_type**: \"children\", \"Govt_jov\", \"Never_worked\", \"Private\" or \"Self-employed\"\n",
                "8. **Residence_type**: \"Rural\" or \"Urban\"\n",
                "9. **avg_glucose_level**: average glucose level in blood\n",
                "10. **bmi**: body mass index\n",
                "11. **smoking_status**: \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"*\n",
                "12. **stroke**: 1 if the patient had a stroke or 0 if not  \n",
                "* Note: \"Unknown\" in smoking_status means that the information is unavailable for this patient"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## **Tasks:**\n",
                "1. Import the data\n",
                "2. Print the shape of the dataset and the first 5 lines\n",
                "3. Calculate the ratio stroke/non-stroke\n",
                "4. Plot histogram of columns separate by stroke vs non-stroke status.\n",
                "5. Print the type of each columns (number, object...)"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## **Questions:**\n",
                "1. How many entries (patients) are in the dataset ?\n",
                "2. How many columns (features) ?\n",
                "3. Plot the histogram of the age feature. Do separate histogram for stroke vs non-stroke patients\n",
                "4. What is the percentage of the patients that had a stroke ?\n",
                "5. Show the type of data in each columns. What type of processing will we have to do for each type ?"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "import warnings\n",
                "warnings.filterwarnings('ignore')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# PART 1: import the data and explore the data.\n",
                "import pandas as pd\n",
                "df = pd.read_csv(\"healthcare-dataset-stroke-data.csv\")\n",
                "df.set_index(\"id\", inplace=True)\n",
                "print(df.shape)\n",
                "df.head()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "print(df[\"stroke\"].value_counts(normalize=True))\n",
                "df.hist(\"age\", by=\"stroke\")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "df.dtypes"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## **Tasks**\n",
                "You will now do the pre-processing of data.\n",
                "For each type of object you will have to process them in a usable format for ML algorithm.\n",
                "\n",
                "## **Questions**\n",
                "1. What columns are categorical data, what columns are numeric.\n",
                "2. What columns are already ready to be used and needs no change.\n",
                "3. What type of processing do you need to do on categorical data and why\n",
                "4. What type of processing do you need to do on numeric data and why\n",
                "5. What columns contains missing data ? What type of processing do you need to do in this case."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "import numpy as np\n",
                "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
                "from sklearn.experimental import enable_iterative_imputer\n",
                "from sklearn.impute import IterativeImputer\n",
                "columns_nothing = [\"hypertension\", \"heart_disease\", \"stroke\"]\n",
                "columns_categorical = [\"gender\",\"ever_married\", \"work_type\", \"Residence_type\", \"smoking_status\"]\n",
                "columns_numeric = [\"age\", \"avg_glucose_level\",\"bmi\"]\n",
                "\n",
                "X_nothing_to_do = df[columns_nothing]\n",
                "X_nothing_to_do = X_nothing_to_do.to_numpy()\n",
                "\n",
                "# Handle Categorial data to numeric (one hot encoding)\n",
                "enc = OneHotEncoder()\n",
                "X_cat = df[columns_categorical]\n",
                "X_cat_onehot = enc.fit_transform(X_cat).toarray()\n",
                "X_cat_columns_onehot = enc.get_feature_names()\n",
                "\n",
                "# Handle Numeric data (scaling)\n",
                "X_num = df[columns_numeric]\n",
                "X_num_scaled = StandardScaler().fit_transform(X_num)\n",
                "X_num_columns_scaled = X_num.columns\n",
                "\n",
                "array_data = np.concatenate((X_cat_onehot, X_num_scaled, X_nothing_to_do), axis=1)\n",
                "\n",
                "# Handle NaN values (Multivariate imputer that estimates each feature from all the others)\n",
                "imp_mean = IterativeImputer(random_state=777)\n",
                "imp_mean.fit(array_data)\n",
                "array_data = imp_mean.transform(array_data)\n",
                "\n",
                "# Recreate the dataframe from all processed-data\n",
                "df_data  = pd.DataFrame(data=array_data, columns=list(X_cat_columns_onehot) + list(X_num_columns_scaled) + columns_nothing)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## **Tasks**\n",
                "Now you can split the data between training and testing data.\n",
                "\n",
                "## **Questions**\n",
                "1. What train/test ratio should you use.\n",
                "2. How many entries are in your train dataset and in your test dataset.\n",
                "3. Verify that you have the same stroke / no-stroke ratio between train and test dataset."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "X = array_data[:,:-1]\n",
                "Y = array_data[:,-1]\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=777)\n",
                "print(X_train.shape)\n",
                "print(X_test.shape)\n",
                "print(np.unique(y_train, return_counts=True)[1])\n",
                "print(np.unique(y_test, return_counts=True)[1])"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# PART 2: Create your machine-learning model"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## **Tasks:**\n",
                "Select from scikit-learn a model and train it (fit) with the train data. Then calculate the accuracy of the model on the test data. Plot the confusion matrix of the test data classification.\n",
                "\n",
                "## **Questions:**\n",
                "1. Which model did you choose and why ? Have you set any particular (hyper)parameters ?\n",
                "2. What accuracy-score do you get and what conclusion can you take ?\n",
                "3. What do you observe on the confusion matrix and what conclusion can you take ?"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import plot_confusion_matrix\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "clf = RandomForestClassifier(class_weight='balanced').fit(X_train, y_train)\n",
                "print(clf.score(X_test, y_test))\n",
                "plot_confusion_matrix(clf, X_test, y_test)  \n",
                "plt.show()  "
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## **Tasks:**\n",
                "You will now try to downsample your majority class to the level of minority class to have a 50/50 ratio and re-do a train/test split.  \n",
                "Then you will re-train a new model with the new ratio-corrected data and get accuracy+confusion matrix plot.\n",
                "\n",
                "## **Questions**\n",
                "1. What accuracy-score do you get with the new model and what conclusion can you take.\n",
                "2. What do you observe on the confusion matrix and what conclusion can you take."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Indicies of each class' observations\n",
                "# Source: https://chrisalbon.com/code/machine_learning/preprocessing_structured_data/handling_imbalanced_classes_with_downsampling/\n",
                "np.random.seed(777)\n",
                "i_class0 = np.where(Y == 0)[0]\n",
                "i_class1 = np.where(Y == 1)[0]\n",
                "# Number of observations in each class\n",
                "n_class0 = len(i_class0)\n",
                "n_class1 = len(i_class1)\n",
                "# For every observation of class 0, randomly sample from class 1 without replacement\n",
                "i_class0_downsampled = np.random.choice(i_class0, size=n_class1, replace=False)\n",
                "# Join together class 0's target vector with the downsampled class 1's target vector\n",
                "Y_down = np.hstack((Y[i_class0_downsampled], Y[i_class1]))\n",
                "X_down = np.vstack((X[i_class0_downsampled], X[i_class1]))\n",
                "\n",
                "# Retrain now with sampled-down data\n",
                "X_train_down, X_test_down, y_train_down, y_test_down = train_test_split(X_down, Y_down, test_size=0.4, random_state=777)\n",
                "clf_down = RandomForestClassifier(class_weight='balanced', random_state=777).fit(X_train_down, y_train_down)\n",
                "print(clf_down.score(X_test_down, y_test_down))\n",
                "plot_confusion_matrix(clf_down, X_test_down, y_test_down)  \n",
                "plt.show()  "
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# PART 3: Evaluate your model"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## **Tasks:**\n",
                "In this last part you will have to calculate all relevant metrics for a binary classification to compare your two models.  \n",
                "Make a table containing the results for both models in terms of: accuracy, balance accuracy, F1 Score, sensitivity (recall), specificity, Precision and confusion matrix data (True Pos., True Neg., False Pos., False Neg.)\n",
                "\n",
                "## **Questions:**\n",
                "1. Which model have the accuracy ?\n",
                "2. Which model have the best area under the curve (AUC) for the ROC-curve ?\n",
                "3. Which model have the best F1-Score and sensitivity ?\n",
                "4. Eventually, which model is better according to you ?"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Calculate all relevant metrics for a binary classification\n",
                "# Accuracy, Balanced Accuracy, F1 Score, Sensitivity (Recall), Specificity, Precision, TP TN FP FN\n",
                "# Also plot the ROC Curve and the Precision-Recall curve and AUC.\n",
                "from sklearn.metrics import accuracy_score\n",
                "from sklearn.metrics import balanced_accuracy_score\n",
                "from sklearn.metrics import recall_score\n",
                "from sklearn.metrics import confusion_matrix\n",
                "from sklearn.metrics import precision_score\n",
                "from sklearn.metrics import f1_score\n",
                "from sklearn.metrics import precision_recall_curve, average_precision_score, plot_precision_recall_curve\n",
                "from sklearn.metrics import roc_curve, auc\n",
                "\n",
                "def roc_plot_single(clf, X_test, y_test):\n",
                "\t#Plot the ROC Curve and include AUC in figure.\n",
                "\tprobas = clf.predict_proba(X_test)\n",
                "\tfpr, tpr, thresholds = roc_curve(y_test, probas[:, 1])\n",
                "\troc_auc = auc(fpr, tpr)\n",
                "\tplt.figure()\n",
                "\tlw = 2\n",
                "\tplt.plot(fpr, tpr, color='darkorange',lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
                "\tplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
                "\tplt.xlim([0.0, 1.0])\n",
                "\tplt.ylim([0.0, 1.05])\n",
                "\tplt.xlabel('False Positive Rate')\n",
                "\tplt.ylabel('True Positive Rate')\n",
                "\tplt.title('ROC Curve')\n",
                "\tplt.legend(loc=\"lower right\")\n",
                "\tplt.show()\n",
                "\n",
                "def get_all_metrics(clf, X_test, y_test):\n",
                "\ty_pred = clf.predict(X_test)\n",
                "\n",
                "\t#calculate and store evaluation metrics\n",
                "\ttn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
                "\n",
                "\tac = accuracy_score(y_test, y_pred)\n",
                "\t# down_weight = [6.24 if x == 1 else 1 for x in y_test_down]\n",
                "\t#bac = balanced_accuracy_score(y_test_down, y_pred, sample_weight=down_weight)\n",
                "\tbac = balanced_accuracy_score(y_test, y_pred)\n",
                "\tre = recall_score(y_test, y_pred)\n",
                "\tpr = precision_score(y_test, y_pred)\n",
                "\tf1 = f1_score(y_test, y_pred)\n",
                "\n",
                "\t#calculate specificity\n",
                "\tif tn == 0 and fp == 0:\n",
                "\t\tsp = 0\n",
                "\telse:\n",
                "\t\tsp = tn/float(tn+fp)\n",
                "\n",
                "\treturn [bac, ac, f1, re, sp, pr, tp, tn, fp, fn]\n",
                "\n",
                "def make_table_clf(clf, clf_down, X_test, y_test, X_test_down, y_test_down):\n",
                "\tresults = get_all_metrics(clf, X_test, y_test)\n",
                "\tresults_down = get_all_metrics(clf_down, X_test_down, y_test_down)\n",
                "\tdf = pd.DataFrame([[i for i in results], [j for j in results_down]], columns=[\"Balanced-Accuracy\", \"Accuracy\", \"F1-Score\", \"Sensitivity (Recall)\", \"Specificity\", \"Precision\", \"TP\",\" TN\", \"FP\", \"FN\"], index=[\"CLF\",\"CLF Down\"])\n",
                "\treturn df\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "make_table_clf(clf, clf_down, X_test, y_test, X_test_down, y_test_down)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "plot_precision_recall_curve(clf, X_test, y_test)\n",
                "roc_plot_single(clf, X_test, y_test)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "plot_precision_recall_curve(clf_down, X_test_down, y_test_down)\n",
                "roc_plot_single(clf_down, X_test_down, y_test_down)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# BONUS 1: Do Cross-validation instead of simple test/train split."
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## **Tasks:**  \n",
                "Instead of a simple Test/Train split, we will do cross-validation. This means that we will train multiple models with different splits, so that all data have been used for training and all for testing. Then we will average the results of all models.  \n",
                "For example, if we do a 80/20% train/test split, then we would need to do a 5 fold cross-validation so that each data has been in the test-set at least once.  \n",
                "Find a way to do \"stratified k fold\", calculate \"cross val score\" with scikit and print the mean and standard deviation of the score.\n",
                "## **Questions:**  \n",
                "1. What is the point of cross-validation ? Did it increase performance ? If not, what is it useful for ?"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "cv = StratifiedKFold(n_splits=5, shuffle=True)\n",
                "cross_scores = cross_val_score(\n",
                "    clf_down,\n",
                "    X_train_down,\n",
                "    y_train_down,\n",
                "    cv=cv,\n",
                "    scoring=\"f1\",\n",
                "    error_score=\"raise\",\n",
                ")\n",
                "print(cross_scores)\n",
                "print(np.mean(cross_scores))\n",
                "print(np.std(cross_scores))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# BONUS 2:  Automatic hyper-parameters tuning"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## **Tasks:**\n",
                "We will use optuna to automatically find the best parameters for the chosen ML Algorithm (Random Forest here). We define a number of trials (50), a scoring metrics to maximize (F1 Score) and a parameters space to explore (params_grid).  \n",
                "In the end we will have a model with the best parameters that have been found and we will compared its metrics to our previous classifier (clf_down) without optimisation.\n",
                "## **Questions:**  \n",
                "1. What are the best parameters detected ? Are your best parameters different from the one of other students ? Why ?\n",
                "2. Did the metrics improved ? Was the optimisation useful ?"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "import optuna\n",
                "from sklearn.base import clone\n",
                "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
                "\n",
                "def make_table_clf_opti(clf, clf_down, X_test, y_test, X_test_down, y_test_down):\n",
                "\tresults = get_all_metrics(clf, X_test, y_test)\n",
                "\tresults_down = get_all_metrics(clf_down, X_test_down, y_test_down)\n",
                "\tdf = pd.DataFrame([[i for i in results], [j for j in results_down]], columns=[\"Balanced-Accuracy\", \"Accuracy\", \"F1-Score\", \"Sensitivity (Recall)\", \"Specificity\", \"Precision\", \"TP\",\" TN\", \"FP\", \"FN\"], index=[\"CLF Down\",\"CLF Down Optimized\"])\n",
                "\treturn df\n",
                "\n",
                "def objective_RF(\n",
                "    trial, est, x_train, y_train, param_grid, scoring_metric):\n",
                "    \"\"\"\n",
                "    A function that is used by Optuna to get the scoring of a model given the parameters of a specific trial.\n",
                "    \"\"\"\n",
                "    params = {\n",
                "        \"n_estimators\": trial.suggest_int(\n",
                "            \"n_estimators\", param_grid[\"n_estimators\"][0], param_grid[\"n_estimators\"][1]\n",
                "        ),\n",
                "        \"criterion\": trial.suggest_categorical(\"criterion\", param_grid[\"criterion\"]),\n",
                "        \"max_depth\": trial.suggest_int(\n",
                "            \"max_depth\", param_grid[\"max_depth\"][0], param_grid[\"max_depth\"][1]\n",
                "        ),\n",
                "        \"min_samples_split\": trial.suggest_int(\n",
                "            \"min_samples_split\",\n",
                "            param_grid[\"min_samples_split\"][0],\n",
                "            param_grid[\"min_samples_split\"][1],\n",
                "        ),\n",
                "        \"min_samples_leaf\": trial.suggest_int(\n",
                "            \"min_samples_leaf\",\n",
                "            param_grid[\"min_samples_leaf\"][0],\n",
                "            param_grid[\"min_samples_leaf\"][1],\n",
                "        ),\n",
                "        \"max_features\": trial.suggest_categorical(\n",
                "            \"max_features\", param_grid[\"max_features\"]\n",
                "        ),\n",
                "        \"bootstrap\": trial.suggest_categorical(\"bootstrap\", param_grid[\"bootstrap\"]),\n",
                "        \"oob_score\": trial.suggest_categorical(\"oob_score\", param_grid[\"oob_score\"]),\n",
                "        \"n_jobs\": trial.suggest_categorical(\"n_jobs\", param_grid[\"n_jobs\"]),\n",
                "        \"class_weight\": trial.suggest_categorical(\n",
                "            \"class_weight\", param_grid[\"class_weight\"]\n",
                "        ),\n",
                "    }\n",
                "    cv = StratifiedKFold(n_splits=5, shuffle=True)\n",
                "    model = clone(est).set_params(**params)\n",
                "    performance = np.mean(\n",
                "        cross_val_score(\n",
                "            model,\n",
                "            x_train,\n",
                "            y_train,\n",
                "            cv=cv,\n",
                "            scoring=scoring_metric,\n",
                "            error_score=\"raise\",\n",
                "        )\n",
                "    )\n",
                "    return performance\n",
                "\n",
                "#Random Forest parameters space\n",
                "param_grid = {'n_estimators': [10,400],\n",
                "                'criterion' : ['gini', 'entropy'],\n",
                "                'max_depth' : [1, 15],\n",
                "                'min_samples_split' : [2, 50], \n",
                "                'min_samples_leaf' : [1, 50],\n",
                "                'max_features' : [None, 'auto','log2'],\n",
                "                'bootstrap' : [True],\n",
                "                'oob_score' : [False, True],\n",
                "                'n_jobs' : [-1],\n",
                "                'class_weight' : [None, 'balanced']}\n",
                "\n",
                "# Run Hyperparameter sweep: first we set up some settings.\n",
                "n_trials = 50\n",
                "scoring_metric = \"f1\"\n",
                "timeout = 300 \n",
                "est = RandomForestClassifier()\n",
                "sampler = optuna.samplers.TPESampler()\n",
                "study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
                "optuna.logging.set_verbosity(optuna.logging.CRITICAL)\n",
                "\n",
                "# We launch the optimization using the previous function as score-returning.\n",
                "study.optimize(\n",
                "    lambda trial: objective_RF(\n",
                "        trial, est, X_train_down, y_train_down, param_grid, scoring_metric),\n",
                "    n_trials=n_trials,\n",
                "    timeout=timeout,\n",
                "    catch=(ValueError,),\n",
                ")\n",
                "# We retrieve the best results now that the optimization is finished.\n",
                "print(\"Best trial:\")\n",
                "best_trial = study.best_trial\n",
                "print(\"  Score: \", best_trial.value)\n",
                "print(\"  Params: \")\n",
                "for key, value in best_trial.params.items():\n",
                "    print(\"    {}: {}\".format(key, value))\n",
                "\n",
                "# Train model using 'best' hyperparameters\n",
                "est = RandomForestClassifier()\n",
                "clf = clone(est).set_params(**best_trial.params)\n",
                "\n",
                "model = clf.fit(X_train_down, y_train_down)\n",
                "y_pred = clf.predict(X_test_down)\n",
                "\n",
                "plot_precision_recall_curve(model, X_test_down, y_test_down)\n",
                "roc_plot_single(model, X_test_down, y_test_down)\n",
                "make_table_clf_opti(clf_down, model, X_test_down, y_test_down, X_test_down, y_test_down)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Additional Ressources\n",
                "\n",
                "If you want to become an AI Expert: AI Rodmap [https://i.am.ai/roadmap](https://i.am.ai/roadmap)  \n",
                "Visualisation in Python [https://www.python-graph-gallery.com/](https://www.python-graph-gallery.com/)  \n",
                "Public Dataset to create projects [https://github.com/awesomedata/awesome-public-datasets](https://github.com/awesomedata/awesome-public-datasets)  \n",
                "Machine Learning Competitions with Prize [https://www.kaggle.com/](https://www.kaggle.com/)"
            ],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.9.5",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.9  ('TD_ML': conda)"
        },
        "interpreter": {
            "hash": "b1f4f77d01ceeeb3f1b50751faca703c6ff6e53a2ece845b812c7f39fafa84f3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}